Introduction to collocation # Importent in word embedding
	Frequency Based (CNF)
	Mean and Varience (CNF)
	Hypothesis Testing (CNF)

Hidden Markov chain # Language modeling (CNF)

Word Embedding # detailed example + two models ()
	- Frequency Based. 
	- Prediction Based. (Explain Skip-garm and CBOW with figure)

Literature Survey

Design # explain the neuralnet diagram

Languages Used

Implementation

Future Works


Collocation
====================

In corpus linguistics, a collocation is a sequence of words or terms that co-occur more often than would be expected by chance. In phraseology, collocation is a sub-type of phraseme. An example of a phraseological collocation is the expression strong tea. 

There are about six main types of collocations: adjective+noun, noun+noun (such as collective nouns), verb+noun, adverb+adjective, verbs+prepositional phrase (phrasal verbs), and verb+adverb.

20/04/18
------------

The simplelest method of finding collocations in a text corpus is counting. If two words occur togather a lot then there is a chance of being collocation, but it is not necessarily a collocation. for example " new york".

NLP
========

Human language is distinct from all other known animal forms of communication in being compositional. Human language allows speakers to express thoughts in sentences comprising subjects, verbs and objects‚Äîsuch as ‚ÄòI kicked the ball‚Äô‚Äîand recognizing past, present and future tenses. Compositionality gives human language an endless capacity for generating new sentences as speakers combine and recombine sets of words into their subject, verb and object roles.

Natural Language Processign aims to apply computational techniques like information extraction, summerization, word sense disambiguation on textual information to understand the knowledge presented in the text.

Word Embedding
=======================

Simply word embedding aims to convert a word into a numeric value. The idea behind convertion into numeric value such that word with similar features tend to have similar value and words with distinct value represents words with non similar features. Feature includes semantic and syntactic occurences of the word. 

Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much higher dimension. 

Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, and explicit representation in terms of the context in which words appear.

Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.

HMM
=============

A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed. The state space, or set of all possible states, can be anything: letters, numbers, weather conditions, baseball scores, or stock performances.

A Markov chain is a stochastic process, but it differs from a general stochastic process in that a Markov chain must be "memory-less". That is, (the probability of) future actions are not dependent upon the steps that led up to the present state. This is called the Markov property.

In simpler Markov models (like a Markov chain), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the hidden Markov model, the state is not directly visible, but the output (in the form of data or "token" in the following), dependent on the state, is visible. Each state has a probability distribution over the possible output tokens.


From video
-----------------

A temporal pattern is defined as a segment of signals that recurs frequently in the whole temporal signal sequence. For example, the temporal signal sequences could be the movements of head, hand, and body, a piece of music, and so on. The patterns of the body movement represent the habit of a person. The patterns of the music represent the melodic phases. Those patterns encode the characteristics of the original temporal sequence and can be used for data summarization and pattern detection.

Recognition of temporal pattern. Patterns that are unforeseen in time. One such problem is speech recognition problem - speech signals are time varying signals - in implementation purpose speech signal is divided into number of phonems - try to find out in which sequence the phonems occur - based on that recognize the word that has been spoken.
another application is sign language recognition - visible : signs, hidden : words.
another : abnormality in persons movement - visible : body pose, hidden : normal or abnormal movement

patters having temporality can be identified using a finate state machine
set of finate state through which machine may do transition - finate set of output and input symbols.
if machine is at a state si at time instance t, then at t=1 the machine will go to another state depending upon the input that is fed
when it makes a transition it produce an output - 
state of the machine is not observable but the emission or the output is observable
so the goal is to identify the state from the input and output.
this finate state machine with output limited to 0 or 1 only then the machine is finate state automata.
such finate state automata has application in bit sequence detection. bit sequence ditection is useful in communication for synchronization.

omega(w) - hidden states
v - visible states
hmm has number of hidden and visible states. 	

given any state there will be a transition to some of the states including the same state - in two subsequent time states machine can be in same state - (1) = sum of the transition probability of any state is one - (2) also sum of the emission probability any state is one 

a HMM has a specific state called accepting state or visible state or final state. once it reaches the state it cannot come out of the state. And in this state it only emits one of the symbol not any other. (3) = state diagram with final state. 

three central issue that need to be addressed in HMM (video 20.00)
1. Evaluation problem.

when the hmm is specified there is hidden state, visible symbols, transition probabilities and emission probability. Now we have a visible sequence (V^T) . Evaluation problem can be stated as, given the visible state (V^T) and HMM (Theta) then we have to find out what is the probability P(visible_sequence/HMM(thete)) ie, probability of generating that visible state by the given HMM.

in case of speech recognition we can see this problem. if i have c - number of sequences and i have to classify an unknown sequence into this visible sequnce - i need to have c number of HMM for this c number of visible siquence - hmm1 for first sequnce , hmm2 for second sequence and so on - every class/sequence is represented by a HMM 

PROBLEM : for a given sequence what is the probability that this sequence is generated by hmm1, probability that this sequence is generated by hmm2 and so on. 

EXAMPLE :
A phoneme based HMM for say the word ‚Äòcat‚Äô would have /k/ /a/ and /t/ as states. In this approach, we will need to create a HMM for every word in the corpus and train it to with the utterances of the word to strengthen the model. 

During recognition, these HMMs provide us an estimate (via probability score) if given sequence of speech segments matches a string of phonemes. Since a string of phonemes can be mapped to a word, HMMs can used to find the most probable word for speech signal.

A phoneme based word HMM model will not scale for recognizing continuous speech since we will need to create a HMM for every word in the corpus. For good recognition of continuous speech you would need to use contextual triphone based sub-word HMMs.

2. Decoding Problem

what is the most likely hidden state sequence w which had led to the generation of ùúÉ .
we want to find out that sequence w that has led to the generation of V .

3. ** The learnig Problem : How the HMM learn the pattern - If training is not done properlly the classifier cannot do proper training.

How many hidden state this HMM has, how many visible states this HMM has is represented by
a rough structure of HMM. But both transition and emission probabilities not known.
Through a set of given training set sequences, i have to find out this transition probabilities.


Word Embedding
===================

why do we need word embedding ?
in order to process natural language by computer we need representation for language more often representation of words. initially the atomic word representation is used. this atomic representation does not depict how human brain percive a word. when a word is seen or heard by human, a set of neurons are activated forming some sort of pattern. if you consider the pattern activated for car, orange and apple, then between the patterns of apple, orange there is less difference as compaired to the difference in pattern of car and orange. This suggests that orange and apple are more similar than other word pairs. 

The neural network model try to do is compute this similarity and disimilarity based on the word occurance statistics.  


Design
===============

The skip-gram neural network model is actually surprisingly simple in its most basic form. First train a simple neural network with a single hidden layer to perform a certain task. The task is given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random then network will predict probability for every word in our vocabulary of being the "nearby word" that we chose.The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word "Soviet", the output probabilities are going to be much higher for words like "Union" and "Russia" than for unrelated words like "watermelon" and "kangaroo". But then we're not actually going to use that neural network for the probability prediction task. Instead, the goal is to learn the weights of the hidden layer which is the "word vectors" that we're trying to learn.


We can train the neural network to find weights by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence "The quick brown fox jumps over the lazy dog." Window size used in this example is 2. The word highlighted in blue is the input word.

T test - latex removed
===============

Calculation of t score can be demonstrated by following example. Let say the hypothesis H is that "The mean height of a population of men is 158" .
 
 \begin{itemize}
     \item Example
     \begin{itemize}
         \item Mean height = 158 cm.
         \item Sample Size = 200 men
         \item Sample Mean = 168
         \item Sample variance = 2600
     \end{itemize}
     \item t test value = 3.05
     \item t value corresponds to a confidence level of 0.0005 is 2.576
     \item Since the t we got is larger than 2.576, we can reject the null hypothesis with 99.5 percentage confidence.  
     \item Conclusion is that the sample is not drawn from a population with mean 158 cm, and the probability of error is less than 0.5 percentage.
 \end{itemize}



co - occ removed
=========================


The big idea - Similar words tend to occur together and will have similar context for example - Apple is a fruit. Mango is a fruit.

Apple and mango tend to have a similar context i.e fruit.

Before I dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified - Co-Occurrence and Context Window.

Co-occurrence - For a given corpus, the co-occurrence of a pair of words say $w_{1}$ and $w_{2}$ is the number of times they have appeared together in a Context Window.

Context Window - Context window is specified by a number and the direction. 

Now, let us take an example corpus to calculate a co-occurrence matrix.

Corpus = He is not lazy. He is intelligent. He is smart.

